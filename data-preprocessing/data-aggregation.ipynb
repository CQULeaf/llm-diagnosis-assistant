{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "ds =  MsDataset.load('.\\FineTome-100k')\n",
    "ds[1]"
   ],
   "id": "345714b2c09c24fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#from unsloth.chat_templates import standardize_sharegpt\n",
    "#dataset = standardize_sharegpt(ds)\n",
    "#dataset[\"conversation\"][0]"
   ],
   "id": "b0c7b97b58a4a11f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"FreedomIntelligence/huatuo_encyclopedia_qa\" , cache_dir=\"D:\\Downloads\\model\\data\")"
   ],
   "id": "fd733485c30a87e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "messages = []\n",
    "\n",
    "# 读取旧的JSONL文件\n",
    "with open(\"D:\\\\Downloads\\\\model\\\\LLaMA-Factory\\\\data\\\\train_datasets.jsonl\", \"r\", encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的json数据\n",
    "        data = json.loads(line)\n",
    "        input = data[\"questions\"]\n",
    "        output = data[\"answers\"]\n",
    "        message = {\n",
    "            \"system\": \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\",\n",
    "            \"query\": str(input),\n",
    "            \"response\":str(output) ,\n",
    "        }\n",
    "        messages.append(message)\n",
    "\n",
    "# 保存重构后的JSONL文件\n",
    "with open(\"/huatuo_test.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for message in messages:\n",
    "        file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")"
   ],
   "id": "c9cb126a4745ac28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "messages = []\n",
    "\n",
    "# 读取旧的中文医疗数据,带COT\n",
    "with open(\"/data/medical_o1_sft_Chinese.json\", \"r\", encoding='utf-8') as file:\n",
    "       # 直接加载整个JSON数组\n",
    "    data_list = json.load(file)\n",
    "\n",
    "    for data in data_list:\n",
    "        instruction = data[\"Question\"]\n",
    "        input = data[\"Question\"]\n",
    "        output = \"<think>\" + str(data[\"Complex_CoT\"]) + \"</think>\" + str(data[\"Response\"])\n",
    "\n",
    "        message = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": \"\",\n",
    "            \"output\": output,\n",
    "        }\n",
    "        messages.append(message)\n",
    "\n",
    "with open(\"/data/medical_o1_sft_Chinese_train.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for message in messages:\n",
    "        file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "num = 0\n",
    "messages = []\n",
    "\n",
    "# 读取中文医疗数据,不带COT\n",
    "with open(\"/data/train_datasets.jsonl\", \"r\", encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的json数据\n",
    "        data = json.loads(line)\n",
    "        instruction = data[\"questions\"]\n",
    "        input = data[\"questions\"]\n",
    "        output = data[\"answers\"]\n",
    "        message = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": \"\",\n",
    "            \"output\": output,\n",
    "        }\n",
    "        messages.append(message)\n",
    "        num += 1\n",
    "        if num==20000:\n",
    "            break\n",
    "\n",
    "with open(\"/data/train_datasets_train.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for message in messages:\n",
    "        file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "num = 0\n",
    "messages = []\n",
    "# 读取通用医疗数据集\n",
    "with open(\"/data/distill_r1_110k.jsonl\", \"r\", encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的json数据\n",
    "        data = json.loads(line)\n",
    "        instruction = data[\"input\"]\n",
    "        input = data[\"input\"]\n",
    "        if num <= 20000:\n",
    "            output = \"<think>\" + str(data[\"reasoning_content\"]) + \"</think>\" + str(data[\"content\"])\n",
    "        else:\n",
    "            output = data[\"content\"]\n",
    "        message = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": \"\",\n",
    "            \"output\": output,\n",
    "        }\n",
    "        messages.append(message)\n",
    "        num += 1\n",
    "        if num==40000:\n",
    "            break\n",
    "\n",
    "# 保存重构后的JSONL文件\n",
    "with open(\"/data/distill_r1_train.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for message in messages:\n",
    "        file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")"
   ],
   "id": "f88ce1f4efe5c343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "num = 0\n",
    "messages = []\n",
    "\n",
    "# 读取中文医疗数据,不带COT\n",
    "with open(\"/data/train_datasets.jsonl\", \"r\", encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的json数据\n",
    "        data = json.loads(line)\n",
    "        instruction = data[\"questions\"][0][0]\n",
    "        input = data[\"questions\"]\n",
    "        output = data[\"answers\"][0]\n",
    "        message = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": \"\",\n",
    "            \"output\": output,\n",
    "        }\n",
    "        messages.append(message)\n",
    "        num += 1\n",
    "        if num==20000:\n",
    "            break\n",
    "\n",
    "with open(\"/data/train_datasets_train.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for message in messages:\n",
    "        file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")"
   ],
   "id": "a02794c20ca42491",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
