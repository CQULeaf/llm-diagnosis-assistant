{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c11090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部署模型\n",
    "!vllm serve /home/jibing/test4vl/test4Medbench/model/Qwen3-8B \\\n",
    "  --port 8060 \\\n",
    "  --dtype bfloat16 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --cpu-offload-gb 0 \\\n",
    "  --gpu-memory-utilization 0.75 \\\n",
    "  --max-model-len 8126 \\\n",
    "  --enable-prefix-caching \\\n",
    "  --enable-reasoning \\\n",
    "  --reasoning-parser deepseek_r1\\\n",
    "  --enable-auto-tool-choice \\\n",
    "  --tool-call-parser hermes \\\n",
    "  --trust-remote-code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99693929",
   "metadata": {},
   "source": [
    "# MedBench数据集自动评测流程\n",
    "利用已部署的 vllm 模型，对 MedBench 数据集进行自动推理，并生成评测平台要求的提交文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置路径和API\n",
    "medbench_dir = './MedBench'  # 替换为你的MedBench文件夹路径\n",
    "output_dir = './demo4test_output'  # 预测结果输出路径\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "vllm_api_url = 'http://127.0.0.1:8000/v1/chat/completions'  # OpenAI兼容API路径\n",
    "\n",
    "def build_prompt(item):\n",
    "    \"\"\"根据题目类型构造模型输入\"\"\"\n",
    "    q = item['question']\n",
    "    opts = item.get('options', None)\n",
    "    # 针对不同任务类型可自定义prompt\n",
    "    if opts:\n",
    "        options_text = '\\n'.join([f\"{chr(65+i)}. {opt}\" for i, opt in enumerate(opts)])\n",
    "        prompt = f\"{q}\\n{options_text}\\n请直接给出正确答案的选项字母。\"\n",
    "    else:\n",
    "        prompt = f\"{q}\\n请直接输出答案，不要输出其他内容。\"\n",
    "    return prompt\n",
    "\n",
    "def filter_thoughts(text):\n",
    "    \"\"\"\n",
    "    过滤掉<think>...</think>标签包裹的内容，只保留标签外的内容。\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # 移除所有<think>...</think>内容\n",
    "    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "\n",
    "def query_vllm(prompt, model=\"你的模型名称\"):\n",
    "    \"\"\"调用vllm OpenAI兼容API获取模型答案，并过滤<think>内容\"\"\"\n",
    "    headers = {\"Authorization\": \"Bearer EMPTY\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 6000,\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "    response = requests.post(vllm_api_url, headers=headers, json=payload, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    raw_answer = response.json()['choices'][0]['message']['content'].strip()\n",
    "    return filter_thoughts(raw_answer)\n",
    "\n",
    "# 获取可用模型名称（可选，或直接写死模型名）\n",
    "def get_vllm_models():\n",
    "    try:\n",
    "        resp = requests.get(\"http://127.0.0.1:8000/v1/models\")\n",
    "        resp.raise_for_status()\n",
    "        models = resp.json().get(\"data\", [])\n",
    "        return [m[\"id\"] for m in models]\n",
    "    except Exception as e:\n",
    "        print(f\"获取模型列表失败: {e}\")\n",
    "        return []\n",
    "\n",
    "# 只处理_test.jsonl文件\n",
    "test_files = []\n",
    "for root, dirs, files in os.walk(medbench_dir):\n",
    "    for fname in files:\n",
    "        if fname.endswith('_test.jsonl'):\n",
    "            test_files.append((root, fname))\n",
    "\n",
    "# 获取模型名\n",
    "models = get_vllm_models()\n",
    "if not models:\n",
    "    raise RuntimeError(\"未检测到vLLM已加载模型，请检查后端。\")\n",
    "model_name = models[0]  # 或手动指定\n",
    "\n",
    "for root, fname in tqdm(test_files, desc=\"全部文件进度\"):\n",
    "    input_path = os.path.join(root, fname)\n",
    "    rel_dir = os.path.relpath(root, medbench_dir)\n",
    "    output_subdir = os.path.join(output_dir, rel_dir)\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "    output_path = os.path.join(output_subdir, fname)\n",
    "\n",
    "\n",
    "    print(f\"正在测试问题集：{input_path}\")\n",
    "    # 支持断点续跑：已存在的输出文件，跳过已完成的样本\n",
    "    finished_ids = set()\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, 'r', encoding='utf-8') as fout:\n",
    "            for line in fout:\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    # 用question字段唯一标识（如有id字段可用id更好）\n",
    "                    finished_ids.add(item.get('question', ''))\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "    with open(output_path, 'a', encoding='utf-8') as fout:  # 追加写入\n",
    "        for line in tqdm(lines, desc=f\"推理 {fname}\", leave=False):\n",
    "            item = json.loads(line)\n",
    "            # 跳过已完成的样本\n",
    "            if item.get('question', '') in finished_ids:\n",
    "                continue\n",
    "            prompt = build_prompt(item)\n",
    "            try:\n",
    "                answer = query_vllm(prompt, model=model_name)\n",
    "            except Exception as e:\n",
    "                answer = \"\"\n",
    "            item['answer'] = answer\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"所有_test.jsonl数据集推理完成，结果已保存到\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8fc81e",
   "metadata": {},
   "source": [
    "# 打包提交\n",
    "将所有预测结果文件放入 MedBench 文件夹，并压缩为 MedBench.zip，按平台要求提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e468fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('MedBench', 'zip', root_dir=output_dir)\n",
    "print(\"已生成 MedBench.zip，可提交评测平台。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
